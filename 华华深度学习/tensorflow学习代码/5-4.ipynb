{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "#定义神经网络的相关参数\n",
    "INPUT_NODE=784\n",
    "OUTPUT_NODE=10\n",
    "LAYER1_NODE=500\n",
    "\n",
    "#通过tf.get_varibale()函数来获取变量\n",
    "\n",
    "def get_weight_variable(shape, regularizer):\n",
    "    weights = tf.get_variable(\"weights\", shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    if(regularizer != None):\n",
    "        tf.add_to_collection('losses', regularizer(weights))\n",
    "    return weights\n",
    "\n",
    "#定义神经网络的前向传播过程\n",
    "def inference(input_tensor, regularizer):\n",
    "    with tf.variable_scope('layer1'):\n",
    "        weights = get_weight_variable([INPUT_NODE, LAYER1_NODE], regularizer)\n",
    "        biases = tf.get_variable(\"biases\", [LAYER1_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n",
    "\n",
    "    with tf.variable_scope('layer2'):\n",
    "        weights = get_weight_variable([LAYER1_NODE, OUTPUT_NODE], regularizer)\n",
    "        biases = tf.get_variable(\"biases\", [OUTPUT_NODE], initializer=tf.constant_initializer(0.0))\n",
    "        layer2 = tf.matmul(layer1, weights) + biases\n",
    "\n",
    "    return layer2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#训练程序\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#定义神经网络结构的参数\n",
    "BATCH_SIZE=128\n",
    "LEARNING_RATE_BASE=0.8\n",
    "LEARNING_RATE_DECAY=0.99\n",
    "REGULARIZATION_RATE=0.0001\n",
    "TRAINING_STEPS=2001\n",
    "MOVING_AVERAGE_DECAY=0.99\n",
    "MODEL_SAVE_PTH='./aa/'\n",
    "MODE_NAME='mnist_model'\n",
    "def train(mnist):\n",
    "    x=tf.placeholder(tf.float32,[None,INPUT_NODE],name='X')\n",
    "    y_=tf.placeholder(tf.float32,[None,OUTPUT_NODE],name='Y')\n",
    "    \n",
    "    regularizer=tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y=inference(x,regularizer)\n",
    "    global_step=tf.Variable(0,trainable=False)\n",
    "    \n",
    "    #定义损失函数、学习率、滑动平均值以及训练过程\n",
    "    variable_average=tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    variable_averages_op=variable_average.apply(tf.trainable_variables())\n",
    "    cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))\n",
    "    cross_entropy_mean=tf.reduce_mean(cross_entropy)\n",
    "    loss=cross_entropy_mean+tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate=tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples/BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True\n",
    "    )\n",
    "    train_step=tf.train.AdamOptimizer(learning_rate).minimize(loss,global_step)\n",
    "    with tf.control_dependencies([train_step,variable_averages_op]):\n",
    "        train_op=tf.no_op(name='train')\n",
    "    \n",
    "    #初始化Tensorflow持久化类\n",
    "    saver=tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs,ys=mnist.train.next_batch(BATCH_SIZE)\n",
    "            _,loss_value,step=sess.run([train_op,loss,global_step],feed_dict={x:xs,y_:ys})\n",
    "            if(i%100==0):\n",
    "                print(\"After %d training step(s),loss on training batch is %g.\"%(step,loss_value))\n",
    "                saver.save(sess,os.path.join(MODEL_SAVE_PTH,MODE_NAME),global_step)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "After 1 training step(s),loss on training batch is 2.80992.\n",
      "After 101 training step(s),loss on training batch is 88.9508.\n",
      "After 201 training step(s),loss on training batch is 43.4766.\n",
      "After 301 training step(s),loss on training batch is 23.6836.\n",
      "After 401 training step(s),loss on training batch is 16.5619.\n",
      "After 501 training step(s),loss on training batch is 10.4843.\n",
      "After 601 training step(s),loss on training batch is 7.76211.\n",
      "After 701 training step(s),loss on training batch is 5.73705.\n",
      "After 801 training step(s),loss on training batch is 4.70326.\n",
      "After 901 training step(s),loss on training batch is 3.90723.\n",
      "After 1001 training step(s),loss on training batch is 3.94096.\n",
      "After 1101 training step(s),loss on training batch is 3.63851.\n",
      "After 1201 training step(s),loss on training batch is 3.45963.\n",
      "After 1301 training step(s),loss on training batch is 3.35359.\n",
      "After 1401 training step(s),loss on training batch is 3.24773.\n",
      "After 1501 training step(s),loss on training batch is 3.17193.\n",
      "After 1601 training step(s),loss on training batch is 3.069.\n",
      "After 1701 training step(s),loss on training batch is 2.97383.\n",
      "After 1801 training step(s),loss on training batch is 2.9194.\n",
      "After 1901 training step(s),loss on training batch is 2.9261.\n",
      "After 2001 training step(s),loss on training batch is 2.82442.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    mnist=input_data.read_data_sets('/tmp/data/',one_hot=True)\n",
    "    train(mnist)\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
