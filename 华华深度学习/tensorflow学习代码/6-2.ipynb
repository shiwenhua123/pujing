{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "INPUT_NODE=784\n",
    "OUTPUT_NODE=10\n",
    "IMAGE_SIZE=28\n",
    "NUM_CHANNELS=1\n",
    "NUM_LABELS=10\n",
    "\n",
    "CONV1_DEEP=32\n",
    "CONV1_SIZE=5\n",
    "CONV2_DEEP=64\n",
    "CONV2_SIZE=5\n",
    "FC_SIZE=512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(input_tensor,train,regularizer):\n",
    "    with tf.variable_scope('layer1-conv1'):\n",
    "        conv1_weights=tf.get_variable('weights',[CONV1_SIZE,CONV1_SIZE,NUM_CHANNELS,CONV1_DEEP],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases=tf.get_variable('bias',[CONV1_DEEP],initializer=tf.constant_initializer(0.0))\n",
    "        conv1=tf.nn.conv2d(input_tensor,conv1_weights,strides=[1,1,1,1],padding='SAME')\n",
    "        relu1=tf.nn.relu(tf.nn.bias_add(conv1,conv1_biases))\n",
    "    with tf.name_scope('layer2-pool1'):\n",
    "        pool1=tf.nn.max_pool(relu1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    with tf.variable_scope('layer3-conv2'):\n",
    "        conv2_weights=tf.get_variable('weights',[CONV2_SIZE,CONV2_SIZE,CONV1_DEEP,CONV2_DEEP],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases=tf.get_variable('bias',[CONV2_DEEP],initializer=tf.constant_initializer(0.0))\n",
    "        conv2=tf.nn.conv2d(pool1,conv2_weights,strides=[1,1,1,1],padding='SAME')\n",
    "        relu2=tf.nn.relu(tf.nn.bias_add(conv2,conv2_biases))\n",
    "    with tf.name_scope('layer4-pool2'):\n",
    "        pool2=tf.nn.max_pool(relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "        pool_shape=pool2.get_shape().as_list()\n",
    "        nodes=pool_shape[1]*pool_shape[2]*pool_shape[3]\n",
    "        reshaped=tf.reshape(pool2,[pool_shape[0],nodes])\n",
    "        \n",
    "    with tf.variable_scope('layer5-fc1'):\n",
    "        fc1_weights=tf.get_variable('weights',[nodes,FC_SIZE],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if(regularizer!=None):\n",
    "            tf.add_to_collection('losses',regularizer(fc1_weights))\n",
    "        fc1_biases=tf.get_variable('bias',[FC_SIZE],initializer=tf.constant_initializer(0.0))\n",
    "        fc1=tf.nn.relu(tf.matmul(reshaped,fc1_weights)+fc1_biases)\n",
    "        if(train):\n",
    "            fc1=tf.nn.dropout(fc1,0.5)\n",
    "    with tf.variable_scope('layer6-fc2'):\n",
    "        fc2_weights=tf.get_variable('weights',[FC_SIZE,NUM_LABELS],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if(regularizer!=None):\n",
    "            tf.add_to_collection('losses',regularizer(fc2_weights))\n",
    "        fc2_biases=tf.get_variable('bias',[NUM_LABELS],initializer=tf.constant_initializer(0.0))\n",
    "        logit=tf.matmul(fc1,fc2_weights)+fc2_biases\n",
    "    return logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "After 1 training step(s) loss on training batch is 6.60508.\n",
      "After 101 training step(s) loss on training batch is 1.86237.\n",
      "After 201 training step(s) loss on training batch is 1.17289.\n",
      "After 301 training step(s) loss on training batch is 1.09287.\n",
      "After 401 training step(s) loss on training batch is 0.936803.\n",
      "After 501 training step(s) loss on training batch is 1.00035.\n",
      "After 601 training step(s) loss on training batch is 0.849522.\n",
      "After 701 training step(s) loss on training batch is 0.810001.\n",
      "After 801 training step(s) loss on training batch is 0.757437.\n",
      "After 901 training step(s) loss on training batch is 0.887816.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#定义神经网络的相关参数\n",
    "BATCH_SIZE=100\n",
    "LEARNING_RATE_BASE=0.01\n",
    "LEARNING_RATE_DECAY=0.99\n",
    "REGULARIZATION_RATE=0.0001\n",
    "TRAINING_STEPS=1000\n",
    "MOVING_AVERAGE_DECAY=0.99\n",
    "MODEL_SAVE_PATH='./bb/'\n",
    "MODEL_NAME='lemodel'\n",
    "\n",
    "\n",
    "def train(mnist):\n",
    "    x=tf.placeholder(tf.float32,[BATCH_SIZE,IMAGE_SIZE,IMAGE_SIZE,NUM_CHANNELS],name='X')\n",
    "    y_=tf.placeholder(tf.float32,[None,OUTPUT_NODE],name='Y')\n",
    "    regularizer=tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    y=inference(x,True,regularizer)\n",
    "    global_step=tf.Variable(0,trainable=False)\n",
    "    \n",
    "    # 定义损失函数、学习率、滑动平均操作以及训练过程。\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    with tf.control_dependencies([train_step, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    \n",
    "    #初始化持久化类\n",
    "    saver=tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            xs,ys=mnist.train.next_batch(BATCH_SIZE)\n",
    "            reshaped_xs=np.reshape(xs,(BATCH_SIZE,IMAGE_SIZE,IMAGE_SIZE,NUM_CHANNELS))\n",
    "            _,loss_value,step=sess.run([train_op,loss,global_step],feed_dict={x:reshaped_xs,y_:ys})\n",
    "            if(i%100==0):\n",
    "                print(\"After %d training step(s) loss on training batch is %g.\"%(step,loss_value))\n",
    "                saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step)\n",
    "                \n",
    "def main():\n",
    "    mnist=input_data.read_data_sets('/tmp/data/',one_hot=True)\n",
    "    train(mnist)\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Restoring parameters from ./bb/lemodel-901\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (10000,) for Tensor 'y_input:0', which has shape '(?, 10)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ee5283504732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-ee5283504732>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mmnist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/tmp/data/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-ee5283504732>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(mnist)\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mreshaped_xs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNUM_CHANNELS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0maccuracy_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mreshaped_xs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After %s training step(s),test accuracy=%g\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pujing\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\pujing\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1097\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (10000,) for Tensor 'y_input:0', which has shape '(?, 10)'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "def evaluate(mnist):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x=tf.placeholder(tf.float32,[\n",
    "            mnist.test.num_examples,\n",
    "            IMAGE_SIZE,\n",
    "            IMAGE_SIZE,\n",
    "            NUM_CHANNELS],name='X_input')\n",
    "        y_=tf.placeholder(tf.float32,[None,OUTPUT_NODE],name='y_input')\n",
    "        validate_feed={x:mnist.test.images,y_:mnist.test.labels}\n",
    "        global_step=tf.Variable(0,trainable=False)\n",
    "        regularizer=tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "        y=inference(x,False,regularizer)\n",
    "        correct_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "        accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        \n",
    "        variable_averages=tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "        variables_to_restore = variable_averages.variables_to_restore()\n",
    "        saver=tf.train.Saver(variables_to_restore)\n",
    "        n=math.ceil(mnist.test.num_examples/mnist.test.num_examples)\n",
    "        for i in range(n):\n",
    "            with tf.Session() as sess:\n",
    "                ckpt=tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "                    global_step=ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                    xs,ys=mnist.test.next_batch(mnist.test.num_examples)\n",
    "                    reshaped_xs=np.reshape(xs,(mnist.test.num_examples,IMAGE_SIZE,IMAGE_SIZE,NUM_CHANNELS))\n",
    "                    accuracy_score=sess.run(accuracy,feed_dict={x:reshaped_xs,y_:ys})\n",
    "                    print(\"After %s training step(s),test accuracy=%g\"%(global_step,accuracy_score))\n",
    "                else:\n",
    "                    print('No check file found')\n",
    "                    return\n",
    "def main():\n",
    "    mnist=input_data.read_data_sets('/tmp/data/')\n",
    "    evaluate(mnist)\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Restoring parameters from ./bb/lemodel-901\n",
      "After 901 training step(s), test accuracy = 0.9559\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "INPUT_NODE=784\n",
    "OUTPUT_NODE=10\n",
    "IMAGE_SIZE=28\n",
    "NUM_CHANNELS=1\n",
    "NUM_LABELS=10\n",
    "\n",
    "CONV1_DEEP=32\n",
    "CONV1_SIZE=5\n",
    "CONV2_DEEP=64\n",
    "CONV2_SIZE=5\n",
    "FC_SIZE=512\n",
    "\n",
    "\n",
    "#定义神经网络的相关参数\n",
    "BATCH_SIZE=100\n",
    "LEARNING_RATE_BASE=0.01\n",
    "LEARNING_RATE_DECAY=0.99\n",
    "REGULARIZATION_RATE=0.0001\n",
    "TRAINING_STEPS=1000\n",
    "MOVING_AVERAGE_DECAY=0.99\n",
    "MODEL_SAVE_PATH='./bb/'\n",
    "MODEL_NAME='lemodel'\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "def evaluate(mnist):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x = tf.placeholder(tf.float32, [\n",
    "            mnist.test.num_examples,\n",
    "            IMAGE_SIZE,\n",
    "            IMAGE_SIZE,\n",
    "            NUM_CHANNELS],\n",
    "                           name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "        validate_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "        y = inference(x, False, regularizer)\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "        variables_to_restore = variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "        #n = math.ceil(mnist.test.num_examples / LeNet5_train.BATCH_SIZE)\n",
    "        n = math.ceil(mnist.test.num_examples / mnist.test.num_examples)\n",
    "        for i in range(n):\n",
    "            with tf.Session() as sess:\n",
    "                ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                    xs, ys = mnist.test.next_batch(mnist.test.num_examples)\n",
    "                    #xs, ys = mnist.test.next_batch(LeNet5_train.BATCH_SIZE)\n",
    "                    reshaped_xs = np.reshape(xs, (\n",
    "                        mnist.test.num_examples,\n",
    "                        IMAGE_SIZE,\n",
    "                        IMAGE_SIZE,\n",
    "                        NUM_CHANNELS))\n",
    "                    accuracy_score = sess.run(accuracy, feed_dict={x:reshaped_xs, y_:ys})\n",
    "                    print(\"After %s training step(s), test accuracy = %g\" % (global_step, accuracy_score))\n",
    "                else:\n",
    "                    print('No checkpoint file found')\n",
    "                    return\n",
    "\n",
    "# 主程序\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "    evaluate(mnist)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
